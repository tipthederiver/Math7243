{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In this lab, we will be performing exploratory data analysis and simple linear regression on a data set of housing prices from Ames, Iowa. This is a famous data set both for its size and relative completeness and will serve as an introduction to some off the simpler aspects of data visualization, data cleaning, and the kinds of choices you have to make to get to the point of performing a numerical analysis. \n",
    "\n",
    "This lab will start with a brief description of types of data and some practice with data visualization. In the second half, we will perform one variable and multivariable linear regression. \n",
    "\n",
    "This lab is based partially on the excellent tutorial by Lee Clemmer: https://www.kaggle.com/leeclemmer/exploratory-data-analysis-of-housing-in-ames-iowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of data:\n",
    "\n",
    "#### Sigular Datatypes\n",
    "* __Booleans__: Variables that take `True` and `False` as values.\n",
    "* __Integers__: Whole numbers, eg `n = 1232` or `-12`.\n",
    "* __Floating Point Numbers__: Decimal numbers, eg `f = 12.342` or `1.79e+308`.\n",
    "* __Strings__: A list of characters, surrounded by `''` or `\"\"`, eg `'This is a string.'`.\n",
    "\n",
    "#### Types of Collections of Data\n",
    "\n",
    "* __Lists__: Ordered collections of numbers, strings, floats, of other lists. For example, `[]` is the empty list, `[1,2,3]`, `[[1,2],'Sandwich',4]`. Lists are ordered, with the indexing starting at 0 and counting up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[1,2],'Sandwich',4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Dictionaries__: Dictionaries are like lists, but values are stored by a __key__ instead of by an index. A dictionary  is defined by curly braces \n",
    "\n",
    "`{'key1': 1, 'key2':[1,2,], 'key3':'Some Text!'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = {'Color':'Blue', 'Number': 4, 'List': [1,2,3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices with numpy\n",
    "Matrix operations can be found in the `numpy` library. Numpy allows easy and efficient matrix operations, as well as some matrix creation mechanism. To create a $2\\times 2$ matrix, we use a list `[]` of lists of columns \n",
    "\n",
    "`[[col1],[col2],[col3],...]` \n",
    "\n",
    "For example, the matrix\n",
    "\n",
    "$$\n",
    "y = \\left(\\begin{matrix}1&2\\\\3&4\\end{matrix}\\right)\\,,\n",
    "$$\n",
    "you use\n",
    "\n",
    "`y = np.matrix(  [[1,2],[3,4]]  )`\n",
    "\n",
    "The following are useful matrix commands\n",
    "\n",
    "* __Transpose__: `A.T`\n",
    "* __Inverse__: `A.I`\n",
    "* __Matrix of 0's__: To produce a `n` by `m` matrix of `0`'s, `np.zeros([n,m])`.\n",
    "* __Matrix of 1's__: To produce a `n` by `m` matrix of `0`'s, `np.ones([n,m])`.\n",
    "* __Identity Matrix__: For an `n` by `m` identity matrix, `np.eye(n)`.\n",
    "* __Random Matrix__: A uniform random matrix of size `n` by `m`, can be created with `np.random.random([n,m])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random([3,2])\n",
    "y = np.matrix( [[12,3],[1,4]] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying output\n",
    "\n",
    "In the IPython uses two main functions to display output: \n",
    "\n",
    "* The python function `print(x)` function prints the raw text of a variable. Print can also be used to construct strings: try setting a value for `n=` and typing `print(\"There were \", n, \"clones chasing me.\")` below. Note that Python automatically adds a space between each argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The IPython function `display(x)` returns the values of `x`, but possibly along with other formatting information. Notice above that if we print the matrix `y` we just get the values, whereas if we `display` it we get some extra information about what type of variable it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data frames with pandas\n",
    "\n",
    "A data frame is Python's version of a spreadsheet. It is a matrix where you can label columns and rows and retrieve  information by those labels. Jupyter naturally displays dataframes nicely. A data frame can constructed from a dictionary, where the keys are the column names. Dataframes play particularly nicely with the `display` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {'Column 1': [1,2,3,4], 'Column 2': [5,6,7,8]}\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data frame can also be created from a matrix by specifying the column names when the dataframe is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random([2,2])\n",
    "df = pd.DataFrame(x, columns=['Column 1','Column 2'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the data from a dataframe by the column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Column 1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data\n",
    "\n",
    "We will download the Ames, Iowa house pricing data from github. The file itself is a CSV with column headers and so we will use pandas to read it in as a data frame. \n",
    "\n",
    "The first step in any data analysis is to understand the data set at a high level, where its coming from and what it indicates:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "DESCRIPTIVE ABSTRACT: Data set contains information from the Ames Assessor’s Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010.<br><br>\n",
    "\n",
    "SOURCES: <br>\n",
    "Ames, Iowa Assessor’s Office <br><br>\n",
    "\n",
    "VARIABLE DESCRIPTIONS:<br>\n",
    "Tab characters are used to separate variables in the data file. The data has 82 columns which include 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers). <br><br>\n",
    "\n",
    "Source: https://www.openintro.org/stat/data/?data=ames<br>\n",
    "Full Description: http://jse.amstat.org/v19n3/decock/DataDocumentation.txt<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv('https://raw.githubusercontent.com/hjhuney/Data/master/AmesHousing/train.csv')\n",
    "ames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Datasets:\n",
    "\n",
    "After reading the high level description, we are ready to dig into the dataset itself. Just displaying the data gave us some information: A brief look at  the content of the columns along with a description of the size, 1460 rows × 81 columns.\n",
    "\n",
    "There are a few commands particularly useful for first pass data analysis:\n",
    "\n",
    "* `type(x)` returns the data type of a variable `x`. In this case we know `ames` is a dataframe but this may not always be so clear. \n",
    "* `DataFrame.head(n)` a data frame has quite a few built in functions associated with it. The head function returns the first `n` columns of the data frame. To use the head function to display the first 5 rows we use `ames.head(5)`.\n",
    "* `DataFrame.tail(n)` displays the last `n` columns of the dataframe.\n",
    "* `DataFrame.shape` is a varaible defining the dimensions of the dataset. You can retrieve them, or change them by changing this variable. \n",
    "\n",
    "Lets try these out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, we make a couple of immediate observations\n",
    "\n",
    "* Jupyter isn't displaying all 81 of the columns. This is okay, we can get information about them later but it's good to note. \n",
    "* We have a large mixture of variable types, some numerical (__Lotarea__) some categorical (__MSZoning__) and others that could be either (__MSSubClass__). You can always check the documentation. \n",
    "* The value `NaN` standing for \"not a number\" keeps appearing. This usually means there's no information about that feature of that particular property. We will have to deal with `NaN` values later. \n",
    "\n",
    "We can actually get some information about the data type of each feature and how many values are `NaN` by using the `DataFrame.info()` function. We can also at least get a list of column names using `list(ames)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately read off which columns are numbers and which are other objects. All of this will help us in deciding on which variables its meaningful to use regression to predict the housing price. \n",
    "\n",
    "We also see that __PoolQC__, __Fence__, __MiscFeature__ all contain a very small number of non-null objects. A non-null object is any value that isn't `NaN`, so this is saying that we mostly don't have data about __PoolQC__, __Fence__, __MiscFeature__. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Stats Corner:</b> Be warned, being a number does not mean the feature should be treated as numeric. Remember that there are 4 main variable types in statistics:\n",
    "<ul><li><b>Discrete:</b> varaibles that take a finite number of values with no relation between the values. For example, type of fruit.</li>\n",
    "    <li><b>Ordinal:</b> variables whose values for a set with a well ordering. For example, stars on a yelp review. 3 is better than 2 but there is no other quantifiable evaluation we can make.  \n",
    "    <li><b>Interval:</b> continuous variables without a fixed, meaningful 0. For example, temperature in $^\\circ C$ can take a continuous value, but $2^\\circ C$ isn't twice as hot as $1^\\circ C$.\n",
    "    <li><b>Ratio:</b> continuous variables with a fixed, meaningful 0.\n",
    "</ul>\n",
    "Regression might not make sense on discrete variables, and must be more carefully interpreted for ordinal variables. We can however perform regression on binary discrete variables since different choices of order would lead to linear functions differing only by a sign. \n",
    "</div>\n",
    "\n",
    "Finally, lets get some idea of what the target range is. The variable we will try to fit is __SalePrice__ so lets use matplotlib to get an idea of what the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Data\n",
    "\n",
    "We will use `matplotlib` to do most of our plotting. There are some more advanced libraries for producing more complicated graphics objects like maps and network graphs but we will only bring those in when needed. \n",
    "\n",
    "We will start my importing `pyplot` from the `matplotlib` library and using the `hist` function to display a histogram of the sale price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(ames.SalePrice,bins=75, rwidth=.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the axis `ax` refers to the specific picture while `fig` refers to an invisible container around it that may contain many pictures inside it. \n",
    "\n",
    "This is a good start, but we should always try to make our plots meaning but adding annotation. In this case, lets give the plot a title, label the axes and increase the size of the figure a bit.\n",
    "\n",
    "* `ax.set_xlabel(\"X Label\")` sets the x axis label for the picture. We can set the fontsize by including `fontsize=`.\n",
    "* `ax.set_ylabel(\"Y Label\")` sets the y axis label for the picture.\n",
    "* `ax.set_title(\"Title\")` sets the title for the picture. \n",
    "* `fig.set_size_inches(10, 7)` sets the dimensions of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "ax.hist(ames.SalePrice,bins=75, rwidth=.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let get the minimum, maximum and average housing cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore some of the other variables, lets take a look at lot size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(ames.LotArea,bins=75, rwidth=.8)\n",
    "ax.set_xlabel(\"Lot Size\", fontsize=20)\n",
    "ax.set_ylabel(\"Count\", fontsize=20)\n",
    "ax.set_title(\"How big are the lots?\", fontsize=20)\n",
    "fig.set_size_inches(10, 7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are a couple of lots that are so extreme that they're not giving us a very good look at the center. Lets cut out all the value above 50,000 sq ft and plot again. We define a logical index `z=ames.LotArea<50000`, which is just a vector of true and false values. If we put this index into a array of the same size, it will only return that values for which the index `z` is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots for Numerical Features\n",
    "\n",
    "In this lab, we really want to look at the numerical features, and in fact only the ordinal features. To pull out the values we will use another built in function of dataframes, the `select_dtype` function:\n",
    "\n",
    "* `DataFrame.select_dtypes(include=['dtype1','dtype2'])` returns a dataframe that only includes the datatypes `'dtype1'`, `'dtype2'`, etc.\n",
    "\n",
    "In this case we want to extract the numerical columns, so we will include `int64` and `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an pair of features, we can form the scatter plot with `matplotlib`'s `plt.plot` function. Without any extra parameters, `plt.plot` returns a line plot, but we can set it to a scatter plot by including a format string, in this case `'o'` (lowercase o). \n",
    "\n",
    "* `ax.plot(x,y,'o')` Plots points whose $x$ coordinates are given in the first vector and whose $y$ coordinates are given in the second vector. The format string `'o'` indicates a scatter plot. \n",
    "\n",
    "To see some other options for formatting, take a look through matplotlibs own tutorial here: https://matplotlib.org/tutorials/introductory/pyplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "ax.plot(nums.LotArea,nums.SalePrice,'o')\n",
    "ax.set_xlabel(\"Lot Size\", fontsize=20)\n",
    "ax.set_ylabel(\"Sale Price\", fontsize=20)\n",
    "ax.set_title(\"Lots vs Sales\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can cut out the outliers using the logical index `z` from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in fact we may just want to excise them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsc = nums[z]   ## Here, the c stands for \"cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare some of the other data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subplots\n",
    "\n",
    "To compare multiple features at once it's useful to create a matrix of side by side subplots. To create such a matrix, we edit the subplots call to include a shape vector describing the number of subplots we want\n",
    "\n",
    "* `fig, axes = plt.subpolots(m,n)` returns an `m` by `n` matrix of subplots. It stores the reference varisble for each subplot in the matrix of subplots in the list `axes`. It also returns a reference to the overall figure containing the subplots, which we store in `fig`. You can plot to the subplot in position (i,j) with `axes[i,j].plot(xvalues, yvalues)`.\n",
    "\n",
    "We can also specify the names inside the call to each plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, sharey=True)\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "#axes[0,0].plot(numsc.GarageArea,numsc.SalePrice,'o')\n",
    "#axes[1,0].plot(numsc.LotArea,numsc.SalePrice,'o')\n",
    "#axes[0,1].plot(numsc.OverallCond,numsc.SalePrice,'o')\n",
    "#axes[1,1].plot(numsc.YrSold,numsc.SalePrice,'o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to see all scatterplots for all of the variables, properly named. To achive this we will loop through all of the data using a for loop. Using `list(numsc)` and the `len` function we can get the lenth of the list of column names for the numeric columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 38 columns containing numeric data, one of which contains the sales price. This can be roughly a $4\\times 10$ grid. \n",
    "\n",
    "One way to display all of the scatter plots is to loop through the axes using two for loops, one running over the columns `m` and one over the rows `n`. The other way is to flatten the 2d array into a list of axes handlers and just loop through the list. In this case, we'd flatten our $4\\times 10$ grid into a single vector of length 40 using `.reshape()`. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =  np.matrix([[0, 1],\n",
    "                [2, 3],\n",
    "                [4, 5],\n",
    "                [6, 7]])\n",
    "\n",
    "print(A.reshape())\n",
    "print(A.reshape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the grid of scatter plots we start with a 4 by 10 array, reshape it a length 40 array, and finally loop through it, plotting the scatter plots as we go. We do have to call the scatter plots by name, so we will actually use the `i`'th index of the `nnames` vector. \n",
    "\n",
    "We also want to label each axis, we can do this with \n",
    "\n",
    "`axes[i].set_title(nnames[i], fontsize=16)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(10, 4, sharey = True)\n",
    "f.set_size_inches(15,30)\n",
    "f.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean this display up a bit, we can drop `id` from the list of numeric variables as it appears to be essentially random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick glance immediately tells us which variables should be treated as discrete or ordinal (which we have to be more careful about fitting with linear regression) and which should be treated as interval or ratio (which is linear regression's natural setting). \n",
    "\n",
    "There are many type of plots we could produce this way. For exploratory data analysis another type of plot could include frequency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 4)\n",
    "f.set_size_inches(15,8)\n",
    "f.tight_layout()\n",
    "\n",
    "axes = axes.reshape(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plot\n",
    "\n",
    "As a last bit of exploratory analysis before moving on to prediction we will produce and plot the correlation matrix for the numerical features. To do so we will use Seaborn, a library for statistical data visualization built on top of `matplotlib`. There is of course the usual debate in the community about whether you should learn seaborn first, since it's easier and more elegant or if you should learn matplotlib first, since it's more fundamental. \n",
    "\n",
    "* `DataFrame.corr()` is a built in function that compute the correlation between the columns of a dataframe. \n",
    "\n",
    "We will use seaborns `sns.heatmap` function to produce a plot of all of the correlations. Seaborn often does an excellent job of displaying things on its own, but it is also deeply customization. In this case, we'll add a `linewidths=0.05` argument to make the correlation more distinct and use the `cmap=\"magma\"` option to get a slightly more readable color scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig,ax = plt.subplots(figsize=(14, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that __OverallQual__, __TotalBsmtSF__, __1stFlrSF__, __GarageArea__, __GrLivArea__, are all highly correlated with sales price. So high value houses have big garages, and large square footage and are overall of good quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We now want to perform our first predictions using linear regression. We will do a quick one variable fit both using linear algebra and then using the sci-kit learn library and compare the fits. \n",
    "\n",
    "For a single feature, we want to fit a 2 vector of constants $[\\beta_0,\\beta_1]$ to \n",
    "$$\n",
    "Y = \\beta_0 + X\\beta_1 = {X}^T\\beta = [1,X] \\left[ \\begin{matrix} \\beta_0\\\\\\beta_1 \\end{matrix} \\right] \\,.\n",
    "$$\n",
    "The solution will be given by\n",
    "$$\n",
    "\\beta = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\,,\n",
    "$$\n",
    "where $\\mathbf{X}$ is the vector where each column is $[1,x_i]$ contains the $x_i$'s measured feature and $\\mathbf{y}$ is a vector of target values.\n",
    "\n",
    "Lets start by splitting the data into training data and test data. For now, we'll just save the last 200 values as test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = numsc['1stFlrSF'][0:-200]\n",
    "#X_test = numsc['1stFlrSF'][-200:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to append a column of a's to the X vector to account for the intercept term $\\beta_0$. This also have the effect ensuring that $\\mathbf{X}^T\\mathbf{X}$ is actually invertable. On way to do this is to just concatinate the vectors and then reshape them into the proper form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.append(np.ones(1249), X_train, 0)\n",
    "#X = X.reshape(2,1249).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now change `X` and `Y` to matricies and perform compute the regression minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the resulting trend line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how we did on our test set, both with a graph and numerically. Graphically, we can just plot the regression line again the test data scatter plot. We can evaluate the model numerically but computing the residual sum squared\n",
    "$$\n",
    "RSS(\\beta) = \\sum_{i=1}^N (y_i - x_i^T\\beta)^2\\,,\n",
    "$$\n",
    "the root mean square\n",
    "$$\n",
    "RMS(\\beta) = \\left(\\frac{1}{N}(y_i - x_i^T\\beta)^2\\right)^{\\frac12}\\,,\n",
    "$$\n",
    "and the $r^2$ value\n",
    "$$\n",
    "r^2 = 1 - \\frac{RSS(\\beta)}{\\sum_{i=1}^N (y_i - E[y])^2}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use sci-kit learns built in regression function. We import `LinearRegression` from `sklearn.linear_model`, the toolkit of linear models in sci-kit learn. We then set up a linear regression object using\n",
    "\n",
    "`lr = LinearRegression()`\n",
    "\n",
    "An object like a linear regression object is a structure like a dataframe. It stores data, but also has a series of utility function associated with it. It is a self contained machine that we can put data into, turn a crank (by calling functions) and have it process and return that data. In this case, it takes in training data and fits a linear regressor to it. We can then ask it return the parameters of that regressor with \n",
    "\n",
    "* `lr.coef_` Returns the slope coefficients of regression $\\beta_1,\\ldots, \\beta_p$.\n",
    "* `lr.intercept_` Returns the intercept of regression $\\beta_0$.\n",
    "\n",
    "Note, that `LinearRegression` wants the data in the form we've been using in class: the $X$ data should be a $N$ by $p$ matrix of data points and the $y$ data should be a $N$ by 1 column vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearRegression` object contains a in built $r^2$ scoring function: \n",
    "\n",
    "* `LinearRegression.score(X_data,Y_data)` takes an array of input data `X_data` an array of label data `Y_data` and returns the $r^2$ score of the regressor on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(np.array(X_test).reshape(-1,1),np.array(Y_test).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with statsmodels.api\n",
    "\n",
    "Although in this class we will be focusing on sci-kit learn, there's another important statistics library to be aware of, statsmodels.api. For regression in particular, statsmodels.api is just a much better tool, performing all of the statistical analysis you would like. The only caveat is that stats model doesn't naturally fit to a constant term, so we must again add a column of 1's. But statsmodels.api has a function to preform that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "x_train_SM = X_train\n",
    "x_train_SM = sm.add_constant(x_train_SM)\n",
    "ols = sm.OLS(Y_train, x_train_SM)\n",
    "ols_result = ols.fit()\n",
    "# Now you have at your disposition several error estimates, e.g.\n",
    "ols_result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually often takes less computational time to compute a linear regression than it does to plot it. We want to fit the set of training data with all the numerical features to the sale price, but before we do we have to deal with those `NaN` values. For example, if we just try to compute the linear regression now it will return an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = numsc[0:-200]\n",
    "X_test = numsc[-200:]\n",
    "\n",
    "lr.fit(np.array(X_train).reshape(-1,1),np.array(Y_train).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing missing values is fiddly, and should be done with care. Whatever choice you make will effect your fit, and as a result will effect your accuracy. In general, data cleaning is the usually more than half the battle. \n",
    "\n",
    "To find the `NaN` values, we will use the `DataFrame.isnull().sum()$ to return all of the null values, and them sum them along columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that __LotFrontage__, __MasVnrArea__, and __GarageYrBlt__ are the only numerical features with null values. Lets take a look at each of these values individually. After we make a choice for each variable, we will use\n",
    "\n",
    "* `DataFrame.Feature.fillna(Value, inplace=True)` fills all of the `NaN` values in an array with the contents of `Value`. \n",
    "\n",
    "__MasVnrArea__ is \"Masonry veneer area in square feet\". Dumping the data we find a lot of zero values, so its probably safe to assume that 8 values can be set to 0 or the median value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For __GarageYrBlt__, a value of `NaN` probably means there's no garage. Lets check just to make sure by looking at the __GarageArea__ variable for every `NaN` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for __LotFrontage__ we assume that almost every lot has some frontage road so we don't want to set this to 0. There are more sophisticated solutions, but the simplest is just to find the median value and set the null values to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we have fixed all of the `NaN` values. Lets compute the linear fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.insert(lr.coef_.reshape(1,37),0, lr.intercept_, axis=1)\n",
    "pd.DataFrame(beta,columns=[\"Const\"]+list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Find the single variable linear predictor which gives the lowest error on the test set. You must justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
