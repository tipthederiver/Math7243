{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing CNNs with Keras\n",
    "\n",
    "In this lab, we will look at constructing convolutional networks with Keras. Keras drastically simplifies the process of constructing a network by using predefined network models and layers. \n",
    "\n",
    "After installing Keras via `pip install keras` restart your kernel. We will start by building a multilayer perceptron, and then move on to CNNs. Our first data set will be our old friend the MNIST dataset. Lets build a perceptron with a perceptron with one deep layer:\n",
    "\n",
    "<img width = 500 src = \"http://i64.tinypic.com/e980nm.png\">\n",
    "\n",
    "We will use this to classify the MNIST dataset. First, we load in MNIST, normalize the X pixel intensities to be between 0 and 1 (this works better with our activation functions) and then encode the labels $Y$ as one-hot encoded categorical variables. We use the `keras.utils` class to do the one-hot encoding, but you could use `pandas.get_dummies` as well.\n",
    "\n",
    "This tutorial follows closely the excellent tutorials https://nextjournal.com/gkoehler/digit-recognition-with-keras and https://colab.research.google.com/github/csc-training/intro-to-dl/blob/master/day1/keras-mnist-mlp.ipynb#scrollTo=-fGvYAbhVADs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "## MNIST:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "NUM_LABELS = 10\n",
    "\n",
    "## Normalize training data to be between 0 and 1, we have to typecast it as a float to do so.\n",
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "\n",
    "# one-hot encoding:\n",
    "Y_train = np_utils.to_categorical(y_train, NUM_LABELS)\n",
    "Y_test = np_utils.to_categorical(y_test, NUM_LABELS)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('x_train:', x_train.shape)\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a sequential model in Keras we import the `sequential` class from `keras.model` and then the types of layers we will use from `keras.labels`. In this case we will be using __dense__ layer and __activation__ layers. \n",
    "\n",
    "We then define a new model with `model = Sequential()`. We add layers in sequence with `model.add(layer)` and Keras takes care of the connects for us:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "__Dense__ implements the operation: __output = activation(dot(input, kernel) + bias)__ where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "\n",
    "[Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(784,),activation='relu'))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we've constructed out model. To compile it we use `model.compile`. We will use the Adam optimizer, which automatically sets a different learning schedule for each weight. For more information about Adam, see [Jason Brownlee's excelent post](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) or [the original paper of Kingma and Ba](https://arxiv.org/abs/1412.6980). \n",
    "\n",
    "To train the model, use\n",
    "\n",
    "`model.fit(X_train, Y_train,\n",
    "          batch_size=, epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))`\n",
    "\n",
    "* `X_train` is the whole set of training data.\n",
    "* `Y_train` is the whole set of label data.\n",
    "* `batch_size` is size of each training minibatch. Remember that 1 is __stochastic gradient decent__ while 60000 (the size of the whole data set) would be __gradient decent__. \n",
    "* `verbose` sets how much information to output during fitting. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "* `validation_data=()` specifies data to validate on after each training epoch. \n",
    "\n",
    "We will save the output of the training in a variable called `history` for later viewing. \n",
    "\n",
    "https://keras.io/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model and saving metrics in history\n",
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving weights with Keras\n",
    "\n",
    "Unlike pure tensorflow, Keras will track the tensorflow session by default, including keeping the model alive in a session for us. To save out the weights, we use `model.save(FILE_NAME)`. The weights can be recovered by using `model.load_weights`. Be warned: you have to build a model with the same architecture first and then load the weights into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "SAVE_DIR = \"./\"\n",
    "MODEL_NAME = 'keras_mnist.h5'\n",
    "model_path = os.path.join(SAVE_DIR, MODEL_NAME)\n",
    "\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Training\n",
    "\n",
    "Keras also saves the models training history. History store the training accuracy, the validation accuracy and the training loss and the validation loss. Below, we plot them against the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "By increasing the layer size, adding more layers, or training time, try to get the validation error above 97%. You may want to add a dropout layer to speed up performance. To add a dropout layer, just put\n",
    "\n",
    "    model.add(Dropout(P)) \n",
    "\n",
    "after a dense layer, where `P` is the proportion of connections to turn off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Lets try out our perceptron network on something a little more complicated. The fashion MNIST dataset has a very similar structure to MNIST accept that instead of simple hand written digits it contains $28\\times 28$ images of items of clothing:\n",
    "\n",
    "<img src=\"http://i64.tinypic.com/10711u0.png\">\n",
    "\n",
    "We load it as below and process it with the same code as before. Take a moment to poke around the dataset before processing it, the labels are\n",
    "\n",
    "Label|Description|Label|Description\n",
    "--- | --- |--- | ---\n",
    "0|T-shirt/top|5|Sandal\n",
    "1|Trouser|6|Shirt\n",
    "2|Pullover|7|Sneaker\n",
    "3|Dress|8|Bag\n",
    "4|Coat|9|Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 10\n",
    "\n",
    "## Normalize training data to be between 0 and 1, we have to typecast it as a float to do so.\n",
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "\n",
    "# one-hot encoding:\n",
    "Y_train = np_utils.to_categorical(y_train, NUM_LABELS)\n",
    "Y_test = np_utils.to_categorical(y_test, NUM_LABELS)\n",
    "\n",
    "print()\n",
    "print('MNIST data loaded: train:',len(X_train),'test:',len(X_test))\n",
    "print('x_train:', x_train.shape)\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('Y_train:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try running the perceptron network about and note the validation error, you may need to increase the number of epochs to get reasonable results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks\n",
    "\n",
    "My error on the MLP never capped 82%, even with 100 epochs. Can we do better with a CNN? Recall that CNN's are comprised of stacks of convolution layers, activation layers, pooling layers and finally a flattening layer:\n",
    "\n",
    "<table bgcolor=\"#fafafa\"><tr>\n",
    "    <td>__Convolution Layer__</td><td><img width=400 src=\"http://i66.tinypic.com/260r9sw.png\">\n",
    "    </tr>\n",
    "    <td>__Pooling Layer__</td><td><img width=300 src=\"http://i66.tinypic.com/19nxc1.png\">\n",
    "    </tr><tr>\n",
    "    <td>__Flattening Layer__</td><td><img width=100 src=\"http://i65.tinypic.com/oau6vr.png\">\n",
    "    </tr></table>\n",
    "    \n",
    "A convolution layer is defined with    \n",
    "    \n",
    "    `Conv2D(nb_filters, kernel_size,\n",
    "                 padding='valid',\n",
    "                 input_shape=input_shape,\n",
    "                 activation='relu')`\n",
    "                 \n",
    "Where\n",
    "\n",
    "* `nb_filters` number of convolution filters.\n",
    "* `kernel_size` size of each filter, say [5,5] for a $5\\times 5$ filter.\n",
    "* `padding` When we convolve, we tend to lower the image size. We can choose to pad the image back to its original size or not. \n",
    "* `input_shape` shape of the inputed training data, only required for the first layer. \n",
    "* `activation` the activation layer following the convolution layer. \n",
    "\n",
    "For a pooling layer we only specify the pool size:\n",
    "\n",
    "* `MaxPooling2D(pool_size=pool_size)` where `pool_size = [2,2]` down-samples by 2 in each direction. \n",
    "\n",
    "After we down-sample enough, we flatten and feed the network into a dense layer to do the fitting. The final architecture looks like the cartoon from class:\n",
    "\n",
    "<img width= 700 src=\"http://i67.tinypic.com/21eca3q.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "## We renormalize the training data since we do not need to flatten it\n",
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "## We have to add an extra dimension to allow for the multiple images we will be creating\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NUM_LABELS)\n",
    "Y_test = np_utils.to_categorical(y_test, NUM_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create our model. Our convolution layers will have $3\\times 3$ filters followed by downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (7,7),\n",
    "                 padding='valid',\n",
    "                 input_shape=(28, 28,1),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3,3),\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=NUM_LABELS, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit, and plot the results. Each epoch will take 10-100 s depending on your processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    Y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=128,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the model\n",
    "\n",
    "There are two things we would like to understand about a CNN: __what is it doing__ and __what is it not doing__? To answer the first question let open up the box a bit and see what the first few convolution kernels look like. \n",
    "\n",
    "The `model.layers[]` array gives a list of handlers for the model layers in the order given by summary. Note that you can also name your layers and call them that way. We then use `layer.get_weights()` to return the convolution and bias weights for each of the 32 kernel layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "print(weights.shape)\n",
    "\n",
    "plt.imshow(weights[:,:,0,9],cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Display all of the kernels in a grid. In addition, normalize the color scheme so that each image uses the same scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the otherhand, its important to know what we're getting wrong. Lets construct the confusion matrix to discern which images the network has the hardest time classifying. \n",
    "\n",
    "Label|Description|Label|Description\n",
    "--- | --- |--- | ---\n",
    "0|T-shirt/top|5|Sandal\n",
    "1|Trouser|6|Shirt\n",
    "2|Pullover|7|Sneaker\n",
    "3|Dress|8|Bag\n",
    "4|Coat|9|Ankle boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_hat = np.argmax(model.predict(X_test),axis=1)\n",
    "conf_mx = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "## Remove diagonal for better viewing\n",
    "row_sum = conf_mx.sum(axis=1, keepdims=True)\n",
    "nconf_mx = conf_mx/row_sum\n",
    "np.fill_diagonal(nconf_mx,0)\n",
    "\n",
    "labels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
    "\n",
    "sns.heatmap(nconf_mx, xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Find and display examples of misclassified items of clothing. What could you do to classify them more accurately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Model Tuning\n",
    "\n",
    "Modify the CNN model above to improve the classification accuracy, or experiment with the effects of different parameters. If you are interested in the state-of-the-art performance on permutation invariant MNIST, see the papers here \n",
    "\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "and at Yann LeCun's webpage\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Be sure to consult the the Keras documentation at https://keras.io/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Data Augmentation\n",
    "\n",
    "The best way to increase the efficiency of a model is to start with more data. Create a function that takes the Fashion MNIST dataset and flips every image horizontally, effectively doubling your dataset size. Does this improve your model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
