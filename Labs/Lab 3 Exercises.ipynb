{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Exercises: MNIST Dataset\n",
    "\n",
    "The following exercises concern the MNIST Dataset. The first step will be to load it locallay. \n",
    "\n",
    "MNIST can be download from http://yann.lecun.com/exdb/mnist/ or from Kaggle https://www.kaggle.com/c/digit-recognizer/data (if you cannot open .gz files). \n",
    "\n",
    "If you are on Google colab, use \n",
    "\n",
    "`from keras.datasets import mnist`\n",
    "\n",
    "`(x_train, y_train), (x_test, y_test) = mnist.load_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## If your files have been saved locally:\n",
    "MNIST_train = pd.read_csv(\"MNIST_train.csv\")\n",
    "MNIST_test = pd.read_csv(\"MNIST_train.csv\")\n",
    "\n",
    "## If you have keras installed:\n",
    "# from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the lab, we saw several ways a 85% mean accuracy could reflect different distributions on the underlying data. \n",
    "\n",
    "* For each digit, there's a 15% chance it will be misclassified.\n",
    "* There's an equal split of data among all digits. However, 9's are always misclassified as 4's and 0's are misclassified half the time as 8's. \n",
    "* There's an equal split of data among all digits. However, 9's are always misclassified and 0's are misclassified 50% of the time, but it's always as something random.\n",
    "* 85% of the data is 1's and the classifier is just classifying everything as 1. \n",
    "\n",
    "What are some others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix:\n",
    "\n",
    "Compare the confusion matrix for Linear Regression, LDA, QDA and Logistic Regression on the test set. Do the different regressors misclassify the same numbers? Which would you find the most trustworthy? Could we combine them to make a better predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Nearest Neighbors\n",
    "\n",
    "Use sci-kit learns $k$-Nearest Neighbors Classifier to classify the MNIST dataset. You can load the k-NN classifier using \n",
    "\n",
    "`from sklearn.neighbors import KNeighborsClassifier`\n",
    "\n",
    "and the documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). The syntax is\n",
    "\n",
    "* `neigh = KNeighborsClassifier(n_neighbors=n)` Where `n` is the number of nearest neighbors. Data is then fit to the classifier by using `neigh.fit(X, y)` and predicted using `neigh.predict(X)`. \n",
    "\n",
    "How does nearest neighbors compare to our other models? Your answer should mention the dependence on $k$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Write a function to perform a 1-hot encoding of a vector\n",
    "\n",
    "Write a function that takes a length $N$ list `y` of labels with $k$ possible values and returns a matrix with the 1-hot encoding of the list. The following might be useful:\n",
    "\n",
    "* `set(y)` returns a set object with one entry to each label. This is a good way to get the possible values. \n",
    "* `list(set(y))` then reformats the possible labels into an ordered list of $k$ elements.\n",
    "* `labels.index('')` can then be used to get the index of an element.\n",
    "\n",
    "Looping over the vector `y`,  you can then find the in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
