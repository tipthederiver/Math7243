{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Linear Methods in Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we're going to look at linear models in classification. In lecture, we considered three methods of fitting linear decision boundaries: fit by __regression__, fit by __estimating labels by a Gaussian distributions__, and fit by __logistic regression__. \n",
    "\n",
    "We will start by testing out our models for binary classification on the UW Breast Cancer data set. This dataset contains a series of measurements derived from pictures of cells and attempts to classify them as cancerous or not, based on their numerical characteristics. \n",
    "\n",
    "<table>\n",
    "    <tr><td>\n",
    "        <img src=\"http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/92_7241.gif\" width=300px>\n",
    "        </td><td width=200px>\n",
    "            $$\\Rightarrow \\{\\text{Malignant, Benign}\\}$$\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "We will then apply multilabel classification techniques to the MNIST handwriting dataset. The MNIST dataset contains pictures of hand written digits, and attempts to classify them. In MNIST, the feature space is high dimensional while the label space is relatively low, and will serve as a bench mark for many of our later machine learning techniques. \n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><td>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=300px>\n",
    "        </td>\n",
    "        </td><td width=20px>\n",
    "            $$\\Rightarrow$$\n",
    "        </td>\n",
    "        <td width=40px>\n",
    "        <table><tr><td>0</td></tr><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr>\n",
    "        <tr><td>4</td></tr><tr><td>5</td></tr><tr><td>6</td></tr><tr><td>7</td></tr><tr><td>8</td></tr>\n",
    "            <tr><td>9</td></tr>\n",
    "        </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "Cancer cells grow more chaotically than their benign counterparts. Their growth tends to be unstable, nonlinear and  ruptured. (See http://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH/PH709_Cancer/PH709_Cancer7.html)\n",
    "<img src=\"http://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH/PH709_Cancer/Characteristics%20of%20Cancer%20Cells.png\" width=400px>\n",
    "The UW Breast Cancer Dataset (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)) contains hand measured characteristics of cells as a training set and their diagnosis as _benign_ or _malignant_. \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src = \"http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/91_5691.gif\" width = 300>\n",
    "    </td><td>\n",
    "    <img src = \"http://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer_images/92_7241.gif\" width = 300>\n",
    "    </td>\n",
    "</tr></table>\n",
    "\n",
    "The data set contains\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.<br><br>\n",
    "\n",
    "Number of instances: 569 <br><br>\n",
    "\n",
    "Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)<br><br>\n",
    "\n",
    "Attribute Information:<br><br>\n",
    "\n",
    "1) ID number<br>\n",
    "2) Diagnosis (M = malignant, B = benign)<br>\n",
    "\n",
    "\n",
    "3-32) Ten real-valued features are computed for each cell nucleus:<br><br>\n",
    "\n",
    "a) radius (mean of distances from center to points on the perimeter)<br>\n",
    "b) texture (standard deviation of gray-scale values)<br>\n",
    "c) perimeter<br>\n",
    "d) area<br>\n",
    "e) smoothness (local variation in radius lengths)<br>\n",
    "f) compactness (perimeter^2 / area - 1.0)<br>\n",
    "g) concavity (severity of concave portions of the contour)<br>\n",
    "h) concave points (number of concave portions of the contour)<br>\n",
    "i) symmetry<br>\n",
    "j) fractal dimension (\"coastline approximation\" - 1)<br><br>\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names\n",
    "</div>\n",
    "\n",
    "It doesn't say this very well in the documentation, but the first set of 10 features numbers in each row is the __mean__ property for all the cells in the image. The second set of 10 features is the __standard error__ and the third is the \"__worst__\", or most extreme. \n",
    "\n",
    "We start by downloading the data and the names of the columns. Note that the datafile itself is just a list of number, so we need to download the names file separately and merge the dataframes.\n",
    "\n",
    "For an excellent kernel on data visualization for the UWBCD, take a look here: https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization. \n",
    "\n",
    "See also Seaborn's documentation on plotting categorical data: https://seaborn.pydata.org/tutorial/categorical.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data')\n",
    "names = [\"id\",\"diagnosis\",\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\n",
    "         \"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave_points_mean\",\n",
    "         \"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\n",
    "         \"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave\" \"points_se\",\n",
    "         \"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\n",
    "         \"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\n",
    "         \"concavity_worst\",\"concave_points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"]\n",
    "\n",
    "data.columns=names\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "\n",
    "Lets start with an exploratory analysis. First, we see that __diagnosis__ is our target variable and __id__ should be dropped. Let generate a few questions for our exploratory analysis:\n",
    "\n",
    "* What does the data look like for each feature?\n",
    "* What is the proportion of __malignant__ to __benign__ samples?\n",
    "* What does the correlation matrix look like for mean, standard error and worst?\n",
    "* Can we visualize the data in a useful way, as violin plots, box plots or swarm plots? As scatter plots?\n",
    "\n",
    "To start, lets use the DataFrame classes built in `DataFrame.describe()` function:\n",
    "\n",
    "* `DataFrame.describe()` returns the __count__, __mean__, __std__, __min__, __max__ and __quantiles__ for all columns of the dataframe. [Doc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split off diagnosis, and normalize the data into units of standard deviation from the mean. Recall that data frame objects have the following:\n",
    "\n",
    "\n",
    "* `DataFrame.count` Count number of non-NA/null observations.\n",
    "* `DataFrame.max` Maximum of the values in the object.\n",
    "* `DataFrame.min` Minimum of the values in the object.\n",
    "* `DataFrame.mean` Mean of the values.\n",
    "* `DataFrame.std` Standard deviation of the obersvations.\n",
    "* `DataFrame.select_dtypes` Subset of a DataFrame including/excluding columns based on their dtype. \n",
    "* `DataFrame.drop(columns=[])` Drop a list of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Feature Columns X\n",
    "\n",
    "## Set Up Target Variables y\n",
    "\n",
    "## Normalize feature data by centering on the mean and dividing by std\n",
    "\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion of Malignant to Benign\n",
    "\n",
    "The proportion of malignant labels to benign labels can be computed using `DataFrame.value_counts()` and displayed using seaborns `sns.countplot()` [Doc](https://seaborn.pydata.org/generated/seaborn.countplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y.value_counts())\n",
    "sns.countplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M,B = y.value_counts()\n",
    "print(\"Roughly \",M/(M+B),\"malignant to\",B/(M+B),\"benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix\n",
    "\n",
    "Dataframes have a built in correlation matrix function, `DataFrame.corr()` and we can use seaborn to plot the heatmap with \n",
    "\n",
    "* `sns.heatmap(matrix, annot=True,linewidth=.5, fmt='.1f')` The heat map of a matrix `matrix`, annotated by the Pearsons coefficient, with lines between the boxes and format of the labels set to `.1f`, that is \"Floating point notation truncated at 1 decimal place.\" For more about string formatting see for example [A Python 3 string formatting guide](https://www.programiz.com/python-programming/methods/string/format).\n",
    "\n",
    "Lets first look at all of the correlations together, and then the correlations between the __mean__, __standard error__ and __worst__ parameters individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(18, 18))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've set up vectors to select features for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\n",
    "         \"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave_points_mean\",\n",
    "         \"symmetry_mean\",\"fractal_dimension_mean\"]\n",
    "ses = [\"radius_se\",\"texture_se\",\"perimeter_se\",\n",
    "         \"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave\" \"points_se\",\n",
    "         \"symmetry_se\",\"fractal_dimension_se\"]\n",
    "worsts = [\"radius_worst\",\"texture_worst\",\n",
    "         \"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\n",
    "         \"concavity_worst\",\"concave_points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No real surprises here from a geometric standpoint. Could there be a difference between the mean correlations for malignant and benign? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(1,2,figsize=(20, 8))\n",
    "I_m = y==\"M\"\n",
    "I_b = y==\"B\"\n",
    "\n",
    "sns.heatmap(X[means][I_m].corr(),annot=True,linewidth=.5, fmt='.1f',ax=axes[0])\n",
    "sns.heatmap(X[means][I_b].corr(),annot=True,linewidth=.5, fmt='.1f',ax=axes[1])\n",
    "\n",
    "axes[0].set_title(\"Correlation for Malignant\",fontsize = 20)\n",
    "axes[1].set_title(\"Correlation for Benign\",fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is now much less correlated with the mean number of concave points. We don't need to guess, we can make this precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box, Violin and Swarm Plots\n",
    "\n",
    "Box, violin and swarm plots are all ways of trying to get a handle on the difference in feature distribution between the malignant and benign cells. \n",
    "\n",
    "A violin plot displays the conditional distributions next to each other for easy visual comparison. \n",
    "\n",
    "<img src=\"https://seaborn.pydata.org/_images/seaborn-violinplot-4.png\">\n",
    "\n",
    "The violin function works a little differently than other functions we have used. It takes a whole dataframe as an object and then asks us to specify which column contains the categories we want along the x-axis, which column contains the data whose distribution we want summarized, and finally which column contains the information about how the data should be labeled. \n",
    "\n",
    "* `sns.violinplot(x=, y=, hue=, data=data, split=True, inner=\"quart\")` Here, split dtermins weather we will have split violins (as above) or side by side symmetric violins. The `inner = quart` line displays the quartiles one the violin plot. \n",
    "\n",
    "`sns.violinplot` really wants to see the data displayed in the following way:\n",
    "\n",
    "|Color Label|X Category|Y Value|\n",
    "|-----|--------|-----|\n",
    "|(Smoker)|(Days)|(Tip Amount)|\n",
    "|Yes| Sun| 5.40|\n",
    "|No | Fri| 1.27|\n",
    "|Yes| Sat| 4.41|\n",
    "|Yes| Sat| 7.88|\n",
    "\n",
    "\n",
    "\n",
    "For example, the code \n",
    "\n",
    "`sns.violinplot(x=\"diagnosis\", y=\"radius_mean\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")`\n",
    "\n",
    "produces a violin plot for the variable __radius_mean__ colored by __diagnosis__. The include of `x=\"diagnosis\"` indicates that on the $x$-axis we will be splitting the data up by the diagnosis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=\"diagnosis\", y=\"radius_mean\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an annoying feature but if we want to make a single violin like we see above we have to include a dummy category vector `x=`, where all of the category are the same.  A simple way to do this is to just pass all 1's to the category vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.violinplot(x=np.ones(568), y=\"radius_mean\", hue=\"diagnosis\", data=data, split=True, inner=\"quart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display all of the features side by side, we have to create a new dataframe of the form \n",
    "\n",
    "|Color Label|X Category|Y Value|\n",
    "|-----|--------|-----|\n",
    "|(diagnosis)|(Feature Name)|(Value)|\n",
    "|B| radius_mean| 2.13|\n",
    "|B| radius_mean| 1.27|\n",
    "|M| radius_mean|-1.49|\n",
    "|$\\vdots$| $\\vdots$| $\\vdots$|\n",
    "|B| area_mean| -1.32|\n",
    "|M| area_mean| 0.41|\n",
    "\n",
    "To do this, we use the `pandas.melt` function to flatten the dataframe into one long $3\\times 30N$ data frame where the first column is the diagnosis, the second column is the corresponding feature and the third column is the value. First, we concatenate `y` back onto `X` and then we melt it to the proper form. \n",
    "\n",
    "* `pandas.melt(DataFrame, id_var=,var_name=,value_name)` Returns a dataframe of identifier varaibles while all other columns, considered measured variables (value_vars), are \"unpivoted\" to the row axis, leaving just two non-identifier columns, `variable` and `value` (to quote the documentation). [Doc.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html)\n",
    "\n",
    "For us, diagnosis will be the identifier variable, and we call the column of variable names \"features\" and the column of values \"value\". The violin plot is then give by letting the features (read: the variable names) run along the $x$-axis, the feature values be collected on the $y$-axis and the colors be determined by __diagnosis__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "vio = pd.concat([y,X[means]],axis=1)\n",
    "vio = pd.melt(vio,id_vars=\"diagnosis\",\n",
    "                    var_name=\"features\",\n",
    "                    value_name='value')\n",
    "\n",
    "sns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=vio,split=True, inner=\"quart\")\n",
    "\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is quiet a large difference for quite a few of the variables, including __radius_mean__, __area_mean__, __concave_points_mean__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box plots\n",
    "\n",
    "Box plots, like violin plots, compare the differences in distributions for different labels, but they do it in a more numerical way.\n",
    "\n",
    "<img width=600px src=\"https://cdn-images-1.medium.com/max/1600/1*2c21SkzJMf3frPXPAR_gZA.png\"> [Source](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n",
    "\n",
    "Here, __Q1__ is the first quartile boundary, so 25% of the data have values less than __Q1__. The inner line is the __median__ and __Q3__ is the third quartile boundary, so 75% of the data have values less than __Q3__.\n",
    "\n",
    "The `seaborn.boxplot` function uses exactly the same syntax as the `seaborn.violinplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like __concave_points_mean__ is really starting to emerge as a favorite for indicating cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swarm Plots\n",
    "\n",
    "A swarm plot is a representation of all of the data in your dataset in a set of nonoverlapping points. It gives a quick visual of how the points are distributed in a relative fashion. \n",
    "\n",
    "As before `sns.swarmplot` uses the same syntax as `sns.violinplot`, so once we done the work to melt our data along categories we can use seaborn to view it many different ways. \n",
    "\n",
    "Swarm plot may take a second to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for Binary Classifiers\n",
    "\n",
    "We will now begin the actual fitting. For visual simplicity, lets first just consider fitting to two variables, __radius_mean__ and __concavity_mean__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[\"id\",\"diagnosis\"])\n",
    "y = data[\"diagnosis\"]\n",
    "I_m = y==\"M\"\n",
    "I_b = y==\"B\"\n",
    "\n",
    "## Normalize feature data\n",
    "X = (X - X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"radius_mean\",fontsize=20)\n",
    "plt.ylabel(\"concavity_mean\",fontsize=20)\n",
    "plt.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot encoding.\n",
    "We need to encode `y` as a one-hot vector. That is, we assign each label to a positional vector. In this case, let\n",
    "\n",
    "|Label|Vector|\n",
    "|-----|------|\n",
    "|B|[1,0]|\n",
    "|M|[0,1]|\n",
    "\n",
    "There are two ways to do this: using built in tool kit and by hand. We will use pandas built in tools here, in the exercise you will proceed by hand. \n",
    "\n",
    "* `pd.get_dummies(y)` Converts categorical variables into dummy index variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y)\n",
    "display(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform regression using sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the parameter and plot the decision boundary. As usual, let $(x_i,y_i)$ be our training data and let $y_i\\in \\{0,\\ldots, k-1\\}$ in keeping with Pythons convention of labeling from 0. From one perspective, we've fit two linear functions using linear regression\n",
    "\n",
    "$$\n",
    "y_B = y_0 \\approx f_0(X) = {\\beta}_{0,0} +  X_1{\\beta}_{1,0} + X_2{\\beta}_{2,0}\\,\\hspace{3em} \n",
    "y_M = y_1 \\approx f_1(X) = {\\beta}_{0,1} +  X_1{\\beta}_{1,1} + X_2{\\beta}_{2,1}\n",
    "$$\n",
    "\n",
    "We recover a categorical fit by selecting $\\hat y_i =  \\underset{k}{\\text{argmax}} (\\hat{f}_k(x_i))$. \n",
    "\n",
    "To find the decision boundary, we just need to find where $\\hat{f}_0(X) = \\hat{f}_1(X)$. Since these are linear functions, it is easy to solve for the hyperplane\n",
    "\n",
    "$$\n",
    "X_2 = \\frac{(\\hat{\\beta}_{1,1}-\\hat{\\beta}_{1,0})X_1 + \\hat{\\beta}_{0,1} - \\hat {\\beta}_{0,0}}{\\hat{\\beta}_{1,0}-\\hat{\\beta}_{1,1}}\\,.\n",
    "$$\n",
    "\n",
    "In fact, $\\hat f_0 = -\\hat f_1$ for two label linear regression (__exercise__) so we could just solve $\\hat f_0 = 0$, but for multilabel classification this is what generalizes. \n",
    "\n",
    "Extracting the $\\beta$ values from the fit using `lr.coef_` and `lr.intercept_` we can plot the decision boundary on the scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "X1 = X[\"radius_mean\"]\n",
    "X2 = X[\"concavity_mean\"]\n",
    "\n",
    "plt.plot(X1[I_m],X2[I_m],'o',label=\"Malignant\")\n",
    "plt.plot(X1[I_b],X2[I_b],'o',label=\"Benign\",alpha=.5)\n",
    "\n",
    "\n",
    "## We want to make a nice clean line directly across the graph as it was before\n",
    "## The best way to do this is to find the limits of the graph and plot using them \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## We also may also want to color in the side of the decicion boundry we're\n",
    "## Labeling each point. One way to do this is using a mesh grid, and then using\n",
    "## an indexon the equation from before\n",
    "\n",
    "\n",
    "\n",
    "## We now reset the x and y limits to make sure our view is centered tightly\n",
    "## around the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Linear) Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "Recall that in quadratic discriminant analysis, we assume a Gaussian distribution for each label\n",
    "\n",
    "$$\n",
    "y_k \\approx f_k(X) = \\big[(2\\pi)^p |\\mathbf{\\Sigma}| \\big]^{-\\frac12}\\exp\\left(\\,-\\frac12(x-\\mu_k)^T\\mathbf{\\Sigma}^{-1}(x-\\mu_k)  \\,\\right)\\,,\n",
    "$$\n",
    "\n",
    "Where $\\mu$ is the center of the label distribution, $\\mathbf{\\Sigma}$ is the covariance matrix. The discriminant functions are then quadratic, and given by \n",
    "\n",
    "$$\n",
    "\\delta_k(x) = -\\frac12\\log|\\mathbf{\\Sigma}_k| - \\frac12 (x-\\mu_k)^T\\mathbf{\\Sigma}_k^{-1}(x-\\mu_k) + \\log \\pi_k\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Sci-kit learn has a QDA library, but if you need to you can estimate the parameters by <br><br>\n",
    "$\\hat pi_k = N_k/N$, where $N_k$ is the number of observations of $k$. <br>\n",
    "$\\hat\\mu_k  = \\frac{1}{N_k}\\sum_{y_i = k} x_i$ is the mean of $k$ observations.<br>\n",
    "$\\hat{\\mathbf{\\Sigma}} = \\frac{1}{N-K}\\sum_{k=1}^K\\sum_{y_i=k}||x_i - \\hat \\mu_k||^2$ estimates covariance.<br>\n",
    "</div>\n",
    "\n",
    "Using sci-kit learn's `QuadraticDiscriminantAnalysis` class from the `discriminant_analysis` library, we can fit the function as before. You can use the code below to compute the linear decision boundary by just changing the function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to do a lot better than regression, but can we trust it? Indeed, this is scoring itself using a different metric to regression. Regression uses least squares while QDA used mean accuracy (mean number of correct prediction). \n",
    "\n",
    "__Question:__ Which would you expect to be higher fore regression, the $r^2$ score or the mean accuracy?\n",
    "\n",
    "We will now use fit object's `predict` function to generate the background labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "X1 = X[\"radius_mean\"]\n",
    "X2 = X[\"concavity_mean\"]\n",
    "\n",
    "plt.plot(X1[I_m],X2[I_m],'o',label=\"Malignant\")\n",
    "plt.plot(X1[I_b],X2[I_b],'o',label=\"Benign\",alpha=.5)\n",
    "\n",
    "## As before we generate a meshgrid, but now we use qda.predict to guess at the label. \n",
    "\n",
    "xm,xM = plt.xlim()\n",
    "ym,yM = plt.ylim()\n",
    "\n",
    "XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) \n",
    "\n",
    "## We now form a 10000x2 array of the (x,y) coordiantes for each point by reshaping\n",
    "## the XX and YY matricies and pasting them together. We need to feed a Nx2 vector\n",
    "## into the qda.predict function, otherwise it will think we have too many features.\n",
    "## We can reshape it later to get our grid back\n",
    "\n",
    " ## We predict, and reshape back to the origional grid\n",
    "\n",
    "z1 = ZZ == 'M'\n",
    "z2 = ZZ == 'B'\n",
    "\n",
    "plt.plot(XX[z1],YY[z1],',',color=\"C0\")\n",
    "plt.plot(XX[z2],YY[z2],',',color=\"C1\")\n",
    "\n",
    "## We now reset the x and y limits to make sure our view is centered tightly\n",
    "## around the data. \n",
    "\n",
    "plt.xlabel(\"radius_mean\",fontsize=20)\n",
    "plt.ylabel(\"concavity_mean\",fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "ax.set_xlim([xm, xM])\n",
    "ax.set_ylim([ym, yM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision boundary for QDA is more difficult than in the linear case. If you are interested in a well worked out examples the official documentation has one here: https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis\n",
    "\n",
    "Compare the above the LDA here below. We have only changed two things: First, we call `LinearDiscriminantAnalysis` instead of quadratic and second we have added the discriminant line from the regression analysis in. Notice that both match up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "qda = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "qda.fit(X_train, y)\n",
    "\n",
    "print(\"Score: %.3f\"%qda.score(X_train,y))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "X1 = X[\"radius_mean\"]\n",
    "X2 = X[\"concavity_mean\"]\n",
    "\n",
    "plt.plot(X1[I_m],X2[I_m],'o',label=\"Malignant\")\n",
    "plt.plot(X1[I_b],X2[I_b],'o',label=\"Benign\",alpha=.5)\n",
    "\n",
    "## As before we generate a meshgrid, but now we use qda.predict to guess at the label. \n",
    "\n",
    "xm,xM = plt.xlim()\n",
    "ym,yM = plt.ylim()\n",
    "\n",
    "XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) \n",
    "\n",
    "## We now form a 10000x2 array of the (x,y) coordiantes for each point by reshaping\n",
    "## the XX and YY matricies and pasting them together. We need to feed a Nx2 vector\n",
    "## into the qda.predict function, otherwise it will think we have too many features.\n",
    "## We can reshape it later to get our grid back\n",
    "\n",
    "grid=np.concatenate([XX.reshape(-1,1),YY.reshape(-1,1)],axis=1)\n",
    "\n",
    "ZZ = qda.predict(grid).reshape(XX.shape)  ## We predict, and reshape back to the origional grid\n",
    "\n",
    "z1 = ZZ == 'M'\n",
    "z2 = ZZ == 'B'\n",
    "\n",
    "plt.plot(XX[z1],YY[z1],',',color=\"C0\")\n",
    "plt.plot(XX[z2],YY[z2],',',color=\"C1\")\n",
    "\n",
    "plt.plot(u,v,label=\"Decision Boundary\",color=\"black\")\n",
    "\n",
    "## We now reset the x and y limits to make sure our view is centered tightly\n",
    "## around the data. \n",
    "\n",
    "plt.xlabel(\"radius_mean\",fontsize=20)\n",
    "plt.ylabel(\"concavity_mean\",fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "ax.set_xlim([xm, xM])\n",
    "ax.set_ylim([ym, yM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "The logistic regression classifier has logistic discriminant functions\n",
    "\n",
    "$$\n",
    "y_j \\approx \\mathbb{P}(G=j|X=x) = \\frac{\\exp(\\beta_{j,0}+x^T\\beta_j)}{1+\\sum_{\\ell=1}^{K-1}\\exp(\\beta_{\\ell,0} + x^T\\beta_\\ell)}\\,,\\hspace{1em} \\forall j=1,\\ldots, K-1\\,,\n",
    "$$\n",
    "and \n",
    "$$\n",
    "y_K \\approx \\mathbb{P}(G=K|X=x)  = \\frac{1}{1+\\sum_{\\ell=1}^{K-1}\\exp(\\beta_{\\ell,0} + x^T\\beta_\\ell)}\\,.\n",
    "$$\n",
    "\n",
    "Again, sci-kit learn has a built in classifier in `sklearn.linear_model`, the `LogisticRegression` class [Doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "X1 = X[\"radius_mean\"]\n",
    "X2 = X[\"concavity_mean\"]\n",
    "\n",
    "plt.plot(X1[I_m],X2[I_m],'o',label=\"Malignant\")\n",
    "plt.plot(X1[I_b],X2[I_b],'o',label=\"Benign\",alpha=.5)\n",
    "\n",
    "## As before we generate a meshgrid, but now we use qda.predict to guess at the label. \n",
    "\n",
    "xm,xM = plt.xlim()\n",
    "ym,yM = plt.ylim()\n",
    "\n",
    "XX, YY = np.meshgrid(np.linspace(xm,xM, 100),np.linspace(ym,yM, 100)) \n",
    "\n",
    "## We now form a 10000x2 array of the (x,y) coordiantes for each point by reshaping\n",
    "## the XX and YY matricies and pasting them together. We need to feed a Nx2 vector\n",
    "## into the qda.predict function, otherwise it will think we have too many features.\n",
    "## We can reshape it later to get our grid back\n",
    "\n",
    "grid=np.concatenate([XX.reshape(-1,1),YY.reshape(-1,1)],axis=1)\n",
    "\n",
    "ZZ = clf.predict(grid).reshape(XX.shape)  ## We predict, and reshape back to the origional grid\n",
    "\n",
    "z1 = ZZ == 'M'\n",
    "z2 = ZZ == 'B'\n",
    "\n",
    "plt.plot(XX[z1],YY[z1],',',color=\"C0\")\n",
    "plt.plot(XX[z2],YY[z2],',',color=\"C1\")\n",
    "\n",
    "plt.plot(u,v,label=\"Decision Boundary\",color=\"black\")\n",
    "\n",
    "## We now reset the x and y limits to make sure our view is centered tightly\n",
    "## around the data. \n",
    "\n",
    "plt.xlabel(\"radius_mean\",fontsize=20)\n",
    "plt.ylabel(\"concavity_mean\",fontsize=20)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "ax.set_xlim([xm, xM])\n",
    "ax.set_ylim([ym, yM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation with Sci-Kit Learn\n",
    "\n",
    "We see that logistic regression actually does a bit _worse_ than linear regression. This is probably not surprising, we know that linear regression should perform well when only being compared to the dataset itself. We would expect to gain something if we split the data and tried cross validation. We will use the `train_test_split` library from sci-kit learns `model_selection` library.\n",
    "\n",
    "* `train_test_split(X,y, test_size=, random_state)` Splits the `X` and `y` data into four pieces: `X_train`, `X_test`, `y_train`, and `y_test`. You may split by number or by percentage. You may use `random_state` to specify a random seed so that you can recover the splitting. if need be. \n",
    "\n",
    "It's a good exercise to see how the relative prediction accuracy changes as we change the `test_size` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Liner Regression vis Linear Discriminant Analysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "lda.fit(X_train, y_train)\n",
    "print(\"LDA Score: %.3f\"%lda.score(X_test,y_test))\n",
    "\n",
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "qda.fit(X_train, y_train)\n",
    "print(\"QDA Score: %.3f\"%qda.score(X_test,y_test))\n",
    "\n",
    "## Logisitic Regression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"Logistic Regression Score: %.3f\"%clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification\n",
    "\n",
    "Now that we have some classification and visualization tools under our belt, lets turn to a high dimensional problem: Using linear methods to classify the MNIST (Mixed National Institute of Standards and Technology) dataset. MNIST is essentially the \"Hello World\" of machine learning.\n",
    "\n",
    "<table>\n",
    "    <tr><td>\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=300px>\n",
    "        </td>\n",
    "        </td><td width=20px>\n",
    "            $$\\Rightarrow$$\n",
    "        </td>\n",
    "        <td width=40px>\n",
    "        <table><tr><td>0</td></tr><tr><td>1</td></tr><tr><td>2</td></tr><tr><td>3</td></tr>\n",
    "        <tr><td>4</td></tr><tr><td>5</td></tr><tr><td>6</td></tr><tr><td>7</td></tr><tr><td>8</td></tr>\n",
    "            <tr><td>9</td></tr>\n",
    "        </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Our goal with MNIST will be to correctly predict the number for picture. \n",
    "\n",
    "It's worth taking a look at MNIST's home: http://yann.lecun.com/exdb/mnist/ There they have the current best benchmarks on the dataset (of course reproducibility is required). \n",
    "\n",
    "Alternatively, you can download it from Kaggle https://www.kaggle.com/c/digit-recognizer/data (if you cannot open .gz files) or if you are on Google colab, use \n",
    "\n",
    "`from keras.datasets import mnist`\n",
    "\n",
    "`(x_train, y_train), (x_test, y_test) = mnist.load_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If your files have been saved locally:\n",
    "\n",
    "MNIST_train = pd.read_csv(\"MNIST_train.csv\")\n",
    "MNIST_test = pd.read_csv(\"MNIST_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MNIST_train.shape)\n",
    "MNIST_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the data set consists of a label, and list of 784 pixel values (either 0 to 255), forming $28\\times 28$ pictures . Lets split into training features and labels and recast the data as a numpy array to make it easier to call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing one of the pictures is simple enough, we just use `.reshape(28,28)` on a row of `X_train` and then use `plt.imshow` to display the pixels of the matrix as black and white. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a 4, and that is what the label tells us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last step before we get started lets shuffle the data set to makes sure we're not picking up any information from the order. To do this we use `numpy.permutation` to generate a permutation of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_index = np.random.permutation(42000)\n",
    "X_train = X_train[shuffle_index]\n",
    "y_train = y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some visualizations\n",
    "\n",
    "It's hard to visualize high dimensional data. The visualization of high dimensional data is a whole lab in and of itself, but we can extract some interesting information (or at least some clarifying information!). \n",
    "\n",
    "First, lets find the \"average\" examples of each number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through the possible labels, we can construct a grid of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also form the pixel by pixel scatter plot, just to see if there is any structure here. We'll light up two adjacent pixels in the very middle: on the pixels 14 and 15 on row 14, so $n_1 = 28\\times(14-1) + 14 = 378$ and $n_2 = 379$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something going on here, but it's not clear what. Maybe if we could project onto the correct dimension this would yield something but as we see the distribution is nontrivial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Classifiers: Logistic Regression\n",
    "\n",
    "Lets start by trying to fit using logistic regression. There is nothing new that we need to do, the regression functions treat a large amount of data the same way as they treat a small amount of data. With logistic regression we don't even need to worry about one-hot encoding the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite good, so on average we're only missclassifying 15% pictures. Given that there is only one correct label and 10 incorrect ones this is a decent result. \n",
    "\n",
    "Remember as well that we have created a real predictor. For example, we can try to predict the 6001'st element of the MNIST data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New predictions\n",
    "\n",
    "Try something yourself: Using MS paint or another program, create a $28\\times 28$ picture with a black background and draw a which number on it. Save the picture as a bitmap in the same directory as the notebook. \n",
    "\n",
    "We can load this picture in with `plt.imread(\"picname.bmp\")` and use `plt.imshow` to display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the shape of the file, you'll see that it is `im.shape = (28,28,4)`. That means it has 4 channels: Red, Green, Blue and Alpha or RGBA. We've been fitting one channel black and white pictures, so to predict you must extract a single channel of data and reshape it from a matrix into a vector. If the picture is black and white the Red, Green and Blue channels will all contain the same information so we can just extract the zero'th channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did it do? If it's drastically off make sure you're picture is black on white, not white on black. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Our Classifiers: LDA and QDA\n",
    "\n",
    "Lets now compare these to the results for the quadratic and linear discriminant classifiers. First, you should notice that they run significantly faster than regression, and in fact we can crank up the number of training samples quite high. As expected, the more training data points we use the better our testing results are, but although that number shoots upward between 100 and 1000 data points, the rate of change tapers off as we pass 10,000 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "## Liner Regression vis Linear Discriminant Analysis\n",
    "\n",
    "print(\"LDA Score: %.3f\"%lda.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "print(\"QDA Score: %.3f\"%qda.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Finally, we will use straight linear regression. Recall our pipeline for linear regression:\n",
    "\n",
    "* Convert the y values to a one-hot vector.\n",
    "* Fit using linear regression.\n",
    "* Predict using argmax\n",
    "\n",
    "You might be surprised to see how much data we can squeeze through the standard linear classifier. You may even be able to train on the whole data set without any slowdown depending on your processor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we understand the score of this model? Is it just completely missclassifying the test data? Lets use `np.argmax` to predict the score on the first image in the training set. Rather annoyingly, we must use\n",
    "\n",
    "`X_test[0].reshape(1, -1)`\n",
    "\n",
    "to reshape the vector into a row vector since numpy returns single vectors as column vectors by default. But then, we're used to all vectors being column vectors unless otherwise stated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've return the linear predictors for each label, we will use `np.argmax` to return the position of the larges $\\hat{f}_k$. Although by looking we can see it is 1, (remember the indexing starts from 0). We also check the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it got the first one correct, what about the first five? Again, we can use `np.argmax`, although we need to specify that we're taking argmax for down the columns by specifying `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictor actually seems to be doing quite well, so what's going on? The answer of course is that it's returning the r^2 score, which is generally going to be horrible on a classification task. A better measure is to use the mean accuracy that the logistic regression, LDA and QDA classes used. Sci-kit learn has build in scores for most loss functions. \n",
    "\n",
    "Loading `accuracy_score` from `sklearn.metrics`, we predict on whole test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enough data this is comparable to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix \n",
    "\n",
    "The mean accuracy is a useful measure of success, but it doesn't give us any granular information. For example, with a mean accuracy of 15% we could be in any of the following scenarios:\n",
    "\n",
    "* For each digit, there's a 15% chance it will be misclassified.\n",
    "* There's an equal split of data among all digits. However, 9's are always misclassified as 4's and 0's are misclassified half the time as 8's. \n",
    "* There's an equal split of data among all digits. However, 9's are always misclassified and 0's are misclassified 50% of the time, but it's always as something random.\n",
    "* 85% of the data is 1's and the classifier is just classifying everything as 1. \n",
    "* Others?\n",
    "\n",
    "This leads to the idea of __precision__ vs __recall__. For a label $k$, let $Tp_k$ be the number of __true positives__, that is the number of items correctly guessed to have label $k$. Let $Fp_k$ be the number of __false positives__, that is the number of items incorrectly guessed to have label $k$.\n",
    "\n",
    "The __precision__ is \n",
    "\n",
    "$$\n",
    "\\textbf{Precision}_k = \\frac{Tp_k}{Tp_k + Fp_k}\\,,\n",
    "$$\n",
    "\n",
    "the proportion items we predicted to be labeled $k$ that actually were. Let $Fn_k$ be the number of __false negatives__, the is the number of items whose true label was $k$ that were incorrectly labeled. The __recall__ is\n",
    "\n",
    "$$\n",
    "\\textbf{Recall}_k = \\frac{Tp_k}{Tp_k + Fn_k}\\,,\n",
    "$$\n",
    "\n",
    "the proportion of items whose true label is $k$ that are labeled correctly. \n",
    "\n",
    "These concepts can be collected into a __Confusion Matrix__. The confusion matrix summarizes how our predictor labeled test data vs the true labeling. \n",
    "\n",
    "We can import the confusion using `from sklearn.metrics import confusion_matrix`. We will then use our linear predictor to predict the labels on the test set, and use `confusion_matrix(y_true,y_predict)` to get a breakdown of how the data is misclassified. The true labeling is along the vertical axis and the guessed labeling is the horizontal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_predict = np.argmax(lr.predict(X_test),axis=1)\n",
    "\n",
    "conf_mx = confusion_matrix(y_test, y_predict)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the number of missclassifications, lets normalize the rows by dividing by the total number of each true label. We can then remove the diagonal and plot a heat map to see where things are getting misclassified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 9's are getting misclassified as 7's, 5's are getting misclassified as 3's and 8's as 1's.  Analyzing the confusion matrix can often tell you what's going right and wrong with your classification. It can also help to look at the individual mistakes. Let's get the index of training sets that contain 5's being misclassified as 3's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these seem like they should have been correctly classified, but some (like the top right corner) a human would have trouble classifying. That represents a sort of practical upper bound, there are many problems on which 100% accuracy is out of reach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
